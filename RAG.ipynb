{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95feaa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10f3865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='지구의 자전 주기는 약 23시간 56분 4초로, 이것을 하루로 계산하면 24시간이 됩니다. 이 자전 주기에 따라서 지구는 자전하는 동안 주변의 태양, 달 및 별들이 동쪽으로 이동하는 것처럼 보입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 16, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BWvUu1eZMqZkacSiD4uyE6jBLL1gC', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e1dfcd72-3b75-485b-83af-9adb1a542798-0', usage_metadata={'input_tokens': 16, 'output_tokens': 98, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm.invoke(\"지구의 자전 주기는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf68afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구의 자전 주기는 약 24시간, 즉 하루입니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "output_parser = StrOutputParser() # str만 출력\n",
    "\n",
    "chain = prompt | llm | output_parser # |로 체인을 구성한답니다 프롬프트 -> 모델 -> invoke\n",
    "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9572b43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word \"미래\" translates to \"future\" in English.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"translates {korean_word} to English\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"explain {english_word} using oxford dictionary to me in Korean\"\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "chain1.invoke({\"korean_word\":\"미래\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34878ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미래는 예상되거나 예상된 시간이나 상황으로, 이후에 오는 시기 또는 미래의 시대를 가리킵니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = (\n",
    "    {\"english_word\" : chain1}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"korean_word\":\"미래\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1fe70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요. 제 이름은 홍길동이고, 나이는 30살입니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt template 모델에 입력하는 입력문\n",
    "# 문장을 입력받아서 문장을 출력할 때\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template_text = \"안녕하세요. 제 이름은 {name}이고, 나이는 {age}살입니다.\"\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
    "filled_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ff67b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['age', 'language', 'name'], input_types={}, partial_variables={}, template='안녕하세요. 제 이름은 {name}이고, 나이는 {age}살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n{language}로 번역해주세요')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combiend_prompt = (\n",
    "    prompt_template\n",
    "    + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를 수 없습니다.\")\n",
    "    + \"\\n\\n{language}로 번역해주세요\"\n",
    ")\n",
    "\n",
    "combiend_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92fb91c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요. 제 이름은 홍길동이고, 나이는 30살입니다.\\n\\n아버지를 아버지라 부를 수 없습니다.\\n\\n영어로 번역해주세요'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combiend_prompt.format(name=\"홍길동\", age=30, language=\"영어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chatprompttemplate 여러 메세지 입력\n",
    "# 채팅 메세지 리스트형식으로\n",
    "# 역할과 내용 부여\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"), # llm 역할 정의\n",
    "    (\"user\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "message = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b61a126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'태양계에서 가장 큰 행성은 목성입니다. 목성은 태양계에서 가장 큰 질량을 가지고 있으며, 지름도 가장 큽니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"user_input\" : \"태양계에서 가장 큰 행성은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65673123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messageprompt template\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "message = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d825f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. 경복궁\\n2. 남산 서울타워\\n3. 제주도 성산일출봉'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "llm.invoke(\"한국의 대표적인 관광지 3군데를 추천해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a184741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 한국의 대표적인 관광지 3군데를 추천해드리겠습니다.\\n\\n1. 경복궁 (Gyeongbokgung Palace) - 서울에 위치한 경복궁은 조선 시대의 궁궐로서 대한민국에서 가장 유명한 궁궐 중 하나입니다. 아름다운 조선 시대의 건축물과 정원을 감상할 수 있으며, 교대식이나 경축행사 등 다양한 체험 프로그램도 제공됩니다.\\n\\n2. 부산 해운대해수욕장 (Haeundae Beach) - 부산에 위치한 해수욕장으로 한국에서 가장 유명하고 인기 있는 해변 중 하나입니다. 푸른 바다와 백사장이 만드는 풀 바캉스 분위기를 즐기며, 주변의 맛집이나 상점도 즐길 수 있습니다.\\n\\n3. 경주 석굴암 (Bulguksa Temple and Seokguram Grotto) - 경주에 위치한 석굴암은 대한민국의 대표적인 사찰로서 석가탑, 불상, 그리고 아름다운 정원 등이 함께 어우러져 있는 곳입니다. 석굴암과 함께 방문하는 범교사와 불교 문화를 느낄 수 있는 곳입니다.\\n\\n위의 관광지들은 한국을 방문하는 여행객들에게 인기있는 명소들입니다. 즐거운 한국 여행 되시길 바랍니다!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 466, 'prompt_tokens': 59, 'total_tokens': 525, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BX1St8Dp3WH3VKuSf5CDnPx6gG2fv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dc25b486-ebaa-4d41-99a5-a43aafc9f7e6-0', usage_metadata={'input_tokens': 59, 'output_tokens': 466, 'total_tokens': 525, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 여행 전문가입니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "chain = chat_prompt | chat\n",
    "chain.invoke({\"user_input\": \"안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc4fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12wkd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3517: UserWarning: Parameters {'stop', 'frequency_penalty', 'presence_penalty'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='태양계에서 가장 큰 행성은 목성입니다. 목성은 지름이 약 142,984km로 태양계에서 가장 큰 크기를 자랑하는 행성이며, 대표적인 가스 행성 중 하나입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 29, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BYka9FNHr2a84Cs2qra6rbhH8Qe3i', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0784efd3-132a-44cd-b99c-faade5e155a1-0' usage_metadata={'input_tokens': 29, 'output_tokens': 75, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# temperature 생성된 텍스트의 다양성을 조정 값이 작으면 예측가능하고 일관된 출력, 값이 크면 다양하고 예측하기 어려움\n",
    "# max tokens 생성할 최대 토큰 수, 텍스트 길이\n",
    "# Top P 다음 출력의 확률값중 상위 P퍼센트만\n",
    "# Frequnecy Penalty, Presence Penalty 빈도와 존재여부에 따라 등장시키는\n",
    "# Stop Sequences 특정단어나 구절이 등장할 경우 생성을 멈춤\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "params = {\n",
    "    \"temperature\" : 0.7,\n",
    "    \"max_tokens\" : 100,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"frequency_penalty\": 0.5,\n",
    "    \"presence_penalty\" : 0.5,\n",
    "    \"stop\" : [\"\\n\"]\n",
    "}\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", **params, model_kwargs=kwargs)\n",
    "\n",
    "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
    "response = model.invoke(input=question)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1936324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='가장 큰 행성은 목' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 29, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BYkcR5AOsde83eq4VI7Nmbi7PGHnO', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--20880a45-8db2-4864-b854-651ebd2c03f7-0' usage_metadata={'input_tokens': 29, 'output_tokens': 10, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"temperature\" : 0.7,\n",
    "    \"max_tokens\" : 10,\n",
    "}\n",
    "\n",
    "response = model.invoke(input=question, **params)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635e39b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='태양계에서 가장 큰 행성은 목성입니다. 목성은 질량, 부피, 지름 등 모든 관점에서 다른 행성들보다 큽니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 58, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BYkiPYKWb1OoaHCovFjCIBGxpjjzE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f5f6bed0-01bd-45b4-8ffc-16f8ebd59639-0' usage_metadata={'input_tokens': 58, 'output_tokens': 54, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='태양계에서 가장 큰' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 58, 'total_tokens': 68, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BYkiQtjijUCjdCex0icaCbcculUxV', 'service_tier': 'default', 'finish_reason': 'length', 'logprobs': None} id='run--fbd0ee4f-9044-439e-9caf-b61fb6a60c8c-0' usage_metadata={'input_tokens': 58, 'output_tokens': 10, 'total_tokens': 68, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=100)\n",
    "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
    "\n",
    "before_answer = model.invoke(messages)\n",
    "print(before_answer)\n",
    "\n",
    "chain = prompt | model.bind(max_tokens=10)\n",
    "after_answer = chain.invoke({\"user_input\":\"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
    "print(after_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf824ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13299\n",
      "동체의 규범을 총체적으로 어기고 있다면 규범 준수를 위해 좀 더 빠르게 강력한 수단을 이용해야 합니다. 특히 정책 문서에 명시된 원칙을 지키지 않는 것은 대부분의 경우 다른 사용자에게 받아들여지지 않습니다 (다른 분들에게 예외 상황임을 설득할 수 있다면 가능하기는 하지만요). 이는 당신을 포함해서 편집자 개개인이 정책과 지침을 직접 집행 및 적용한다는 것을 의미합니다.\n",
      "특정 사용자가 명백히 정책에 반하는 행동을 하거나 정책과 상충되는 방식으로 지침을 어기는 경우, 특히 의도적이고 지속적으로 그런 행위를 하는 경우 해당 사용자는 관리자의 제재 조치로 일시적, 혹은 영구적으로 편집이 차단될 수 있습니다. 영어판을 비롯한 타 언어판에서는 일반적인 분쟁 해결 절차로 끝낼 수 없는 사안은 중재위원회가 개입하기도 합니다.\n",
      "\n",
      "문서 내용\n",
      "정책과 지침의 문서 내용은 처음 읽는 사용자라도 원칙과 규범을 잘 이해할 수 있도록 다음 원칙을 지켜야 합니다.\n",
      "\n",
      "명확하게 작성하세요. 소수만 알아듣거나 준법률적인 단어, 혹은 지나치게 단순한 표현은 피해야 합니다. 명확하고, 직접적이고, 모호하지 않고, 구체적으로 작성하세요. 지나치게 상투적인 표현이나 일반론은 피하세요. 지침, 도움말 문서 및 기타 정보문 문서에서도 \"해야 합니다\" 혹은 \"하지 말아야 합니다\" 같이 직접적인 표현을 굳이 꺼릴 필요는 없습니다.\n",
      "가능한 간결하게, 너무 단순하지는 않게. 정책이 중언부언하면 오해를 부릅니다. 불필요한 말은 생략하세요. 직접적이고 간결한 설명이 마구잡이식 예시 나열보다 더 이해하기 쉽습니다. 각주나 관련 문서 링크를 이용하여 더 상세히 설명할 수도 있습니다.\n",
      "규칙을 만든 의도를 강조하세요. 사용자들이 상식대로 행동하리라 기대하세요. 정책의 의도가 명료하다면, 추가 설명은 필요 없죠. 즉 규칙을 '어떻게' 지키는지와 더불어 '왜' 지켜야 하는지 확실하게 밝혀야 합니다.\n",
      "범위는 분명히, 중복은 피하기. 되도록 앞부분에서 정책 및 지침의 목적과 범위를 분명하게 밝혀야 합니다. 독자 대부분은 도입부 초\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(len(docs[0].page_content))\n",
    "print(docs[0].page_content[5000:6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a74351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "page_content='제안과 채택\n",
      " 백:아님 § 관료주의  문서를 참고하십시오. 단축백:제안\n",
      "제안 문서란 정책과 지침으로 채택하자고 의견을 묻는 문서이나 아직 위키백과 내에 받아들여지는 원칙으로 확립되지는 않은 문서입니다. {{제안}} 틀을 붙여 공동체 내에서 정책이나 지침으로 채택할 지 의견을 물을 수 있습니다. 제안 문서는 정책과 지침이 아니므로 아무리 실제 있는 정책이나 지침을 요약하거나 인용해서 다른 문서에 쓴다고 해도 함부로 정책이나 지침 틀을 붙여서는 안 됩니다.\n",
      "'제안'은 완전 새로운 원칙이라기보다, 기존의 불문율이나 토론 총의의 문서를 통한 구체화에 가깝습니다. 많은 사람들이 쉽게 제안을 받아들이도록 하기 위해서는, 기초적인 원칙을 우선 정하고 기본 틀을 짜야 합니다. 정책과 지침의 기본 원칙은 \"왜 지켜야 하는가?\", \"어떻게 지켜야 하는가?\" 두 가지입니다. 특정 원칙을 정책이나 지침으로 확립하기 위해서는 우선 저 두 가지 물음에 성실하게 답하는 제안 문서를 작성해야 합니다.\n",
      "좋은 아이디어를 싣기 위해 사랑방이나 관련 위키프로젝트에 도움을 구해 피드백을 요청할 수 있습니다. 이 과정에서 공동체가 어느 정도 받아들일 수 있는 원칙이 구체화됩니다. 많은 이와의 토론을 통해 공감대가 형성되고 제안을 개선할 수 있습니다.\n",
      "정책이나 지침은 위키백과 내의 모든 편집자들에게 적용되는 원칙이므로 높은 수준의 총의가 요구됩니다. 제안 문서가 잘 짜여졌고 충분히 논의되었다면, 더 많은 공동체의 편집자와 논의를 하기 위해 승격 제안을 올려야 합니다. 제안 문서 맨 위에 {{제안}}을 붙여 제안 안건임을 알려주고, 토론 문서에 {{의견 요청}}을 붙인 뒤 채택 제안에 관한 토론 문단을 새로 만들면 됩니다. 많은 편집자들에게 알리기 위해 관련 내용을 {{위키백과 소식}}에 올리고 사랑방에 이를 공지해야 하며, 합의가 있을 경우 미디어위키의 sitenotice(위키백과 최상단에 노출되는 구역)에 공지할 수도 있습니다.' metadata={'source': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8', 'title': '위키백과:정책과 지침 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_spliter.split_documents(docs)\n",
    "\n",
    "print(len(splits))\n",
    "print(splits[10]) # page_content + metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0456d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "격하\n",
      "특정 정책이나 지침이 편집 관행이나 공동체 규범이 바뀌며 쓸모없어질 수 있고, 다른 문서가 개선되어 내용이 중복될 수 있으며, 불필요한 내용이 증식할 수도 있습니다. 이 경우 편집자들은 정책을 지침으로 격하하거나, 정책 또는 지침을 보충 설명, 정보문, 수필 또는 중단 문서로 격하할 것을 제안할 수 있습니다. \n",
      "격하 과정은 채택 과정과 비슷합니다. 일반적으로 토론 문서 내 논의가 시작되고 프로젝트 문서 상단에 {{새로운 토론|문단=진행 중인 토론 문단}} 틀을 붙여 공동체의 참여를 요청합니다. 논의가 충분히 이루어진 후, 제3의 편집자가 토론을 종료하고 평가한 후 상태 변경 총의가 형성되었는지 판단해야 합니다. 폐지된 정책이나 지침은 최상단에 {{중단}} 틀을 붙여 더 이상 사용하지 않는 정책/지침임을 알립니다.\n",
      "소수의 공동체 인원만 지지하는 수필, 정보문 및 기타 비공식 문서는 일반적으로 주된 작성자의 사용자 이름공간으로 이동합니다. 이러한 논의는 일반적으로 해당 문서의 토론란에서 이루어지며, 간혹 위키백과:의견 요청을 통해 처리되기도 합니다.\n",
      "\n",
      "같이 보기\n",
      "위키백과:위키백과의 정책과 지침 목록\n",
      "위키백과:의견 요청\n",
      "수필\n",
      "\n",
      "위키백과:제품, 절차, 정책\n",
      "위키백과:위키백과 공동체의 기대와 규범\n",
      "기타 링크\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "docs = vectorstore.similarity_search(\"격하 과정에 대해서 설명해주세요.\")\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "385ab526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'격하 과정은 특정 정책이나 지침이 더 이상 필요하지 않거나 개선이 필요한 경우, 해당 정책이나 지침을 수정하거나 폐지하기 위해 공동체의 참여를 요청하는 과정입니다. 이 과정은 채택 과정과 유사하며, 토론을 통해 변경 또는 폐지 여부를 결정합니다. 변경이나 폐지된 정책이나 지침은 해당 문서 상단에 표시를 하여 더 이상 사용되지 않음을 알립니다.소수의 공동체 인원만 지지하는 문서는 주된 작성자의 사용자 이름공간으로 이동될 수 있습니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = '''Anwer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Quesrion: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': retriver | format_docs, 'question': RunnablePassthrough()}\n",
    "    |prompt\n",
    "    |model\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"격하 과정에 대해서 설명해주세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
